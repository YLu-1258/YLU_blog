{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: Initial LSTM Java Testing\n",
    "author: Alex, David\n",
    "categories: [Lab Notebook]\n",
    "tags: [Java, LSTM, nd4j, deeplearning4j]\n",
    "description: Testing the training of the AI model with example code\n",
    "toc: True\n",
    "comments: True\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This is our initial test with LSTM. Here we do our initial data collection from Yahoo finance and use it to attempt to train and test our model and it's accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection\n",
    "> Collecting data with Python.\n",
    "\n",
    "We use a Yahoo Finance library to collect the data for the stocks from the previous 20 years. We store the data in a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import yfinance as yf # library for data\n",
    "import pandas as pd\n",
    "tickers = [\"GOOGL\", \"AMZN\", \"AAPL\", \"TSLA\", \"WMT\", \"MSFT\", \"META\", \"COST\", \"LMT\", \"NOC\", \"UNH\"] # tickers for data collecting\n",
    "for ticker in tickers: # iterating through each stock\n",
    "    print(\"Processing ticker: \", ticker)\n",
    "    data = yf.download(ticker, start=\"2004-01-01\", end=\"2024-01-22\") # gathering data\n",
    "    csv_data = data.to_csv(path_or_buf=f\"./stock_data/{ticker}.csv\") # data files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Viewing Data\n",
    "> This is how we view the data that we have gathered, in a more pleasant format.\n",
    "\n",
    "We use a java charting tool to chart the data and display it for future use when comparing our predicted data to our real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chart saved as image: ./stock_charts/date_vs_volume/AAPL_chart.png\n",
      "Chart saved as image: ./stock_charts/date_vs_volume/AMZN_chart.png\n",
      "Chart saved as image: ./stock_charts/date_vs_volume/COST_chart.png\n",
      "Chart saved as image: ./stock_charts/date_vs_volume/GOOGL_chart.png\n",
      "Chart saved as image: ./stock_charts/date_vs_volume/LMT_chart.png\n",
      "Chart saved as image: ./stock_charts/date_vs_volume/META_chart.png\n",
      "Chart saved as image: ./stock_charts/date_vs_volume/MSFT_chart.png\n",
      "Chart saved as image: ./stock_charts/date_vs_volume/NOC_chart.png\n",
      "Chart saved as image: ./stock_charts/date_vs_volume/TSLA_chart.png\n",
      "Chart saved as image: ./stock_charts/date_vs_volume/UNH_chart.png\n",
      "Chart saved as image: ./stock_charts/date_vs_volume/WMT_chart.png\n"
     ]
    }
   ],
   "source": [
    "// libraries and imports needed to create the graphs as images\n",
    "%maven org.jfree:jfreechart:1.5.3\n",
    "%maven com.opencsv:opencsv:5.5\n",
    "import org.jfree.chart.ChartFactory;\n",
    "import org.jfree.chart.ChartUtils;\n",
    "import org.jfree.chart.JFreeChart;\n",
    "import org.jfree.data.category.CategoryDataset;\n",
    "import org.jfree.data.category.DefaultCategoryDataset;\n",
    "import java.io.File;\n",
    "import java.io.BufferedReader;  \n",
    "import java.io.IOException;\n",
    "import java.io.FileReader;\n",
    "\n",
    "public class JFreeChartExample {\n",
    "    // gathering data from the CSV files\n",
    "    public static CategoryDataset createDataset(String ticker){\n",
    "        DefaultCategoryDataset dataset = new DefaultCategoryDataset();\n",
    "        // csv data for stocks\n",
    "        String csvFilePath = \"./stock_data/\" + ticker + \".csv\";\n",
    "        try (BufferedReader br = new BufferedReader(new FileReader(csvFilePath))) {\n",
    "            int dateColumnIndex = 0;\n",
    "            int closeColumnIndex = 6;\n",
    "            String line = br.readLine();  \n",
    "            while ((line = br.readLine()) != null) {\n",
    "                String[] data = line.split(\",\");\n",
    "                String date = data[dateColumnIndex];\n",
    "                Double close = Double.parseDouble(data[closeColumnIndex]);\n",
    "                // iterating through the necessary data in the CSV\n",
    "                dataset.addValue(close, ticker, date);\n",
    "            }\n",
    "        } catch (IOException e) {\n",
    "            e.printStackTrace();\n",
    "        }\n",
    "\n",
    "        return dataset;\n",
    "    }\n",
    "    \n",
    "    // charting the data\n",
    "    public static JFreeChart createChart(CategoryDataset dataset, String ticker, String val1, String val2) {\n",
    "        String title = val1 + \" vs. \" + val2 + \" for \" + ticker;\n",
    "        // creating a visual graph of the data using a charting library\n",
    "        return ChartFactory.createLineChart(\n",
    "                title,\n",
    "                val1,\n",
    "                val2,\n",
    "                dataset\n",
    "        );\n",
    "    }\n",
    "\n",
    "    // saving the chart as an image\n",
    "    public static void saveChartAsImage(JFreeChart chart, String filePath, int width, int height) throws IOException {\n",
    "        ChartUtils.saveChartAsPNG(new File(filePath), chart, width, height);\n",
    "    }\n",
    "\n",
    "    // running classes to graph data\n",
    "    public static void main(String[] args) {\n",
    "        try {\n",
    "            String[] tickers = {\"AAPL\", \"AMZN\", \"COST\", \"GOOGL\", \"LMT\", \"META\", \"MSFT\", \"NOC\", \"TSLA\", \"UNH\", \"WMT\"};\n",
    "            for (String ticker : tickers) {\n",
    "                // running through charting methosd to create images of data\n",
    "                CategoryDataset dataset = createDataset(ticker);\n",
    "                JFreeChart chart = createChart(dataset, ticker, \"Date\", \"Volume\");\n",
    "                String chartname = \"./stock_charts/date_vs_volume/\" + ticker + \"_chart.png\";\n",
    "\n",
    "                // Save chart as image\n",
    "                saveChartAsImage(chart, chartname, 800, 600);\n",
    "\n",
    "                System.out.println(\"Chart saved as image: \" + chartname);\n",
    "            }\n",
    "            \n",
    "        } catch (IOException e) {\n",
    "            e.printStackTrace();\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "JFreeChartExample.main(null);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create testing and training datasets\n",
    "> This is where we begin implementing LSTM\n",
    "\n",
    "First we need to train the LSTM model in order to be able to predict future results. After training the model, we need to test whether our predicted values line up with the actual values of the stock data, allowing us to verify that our data is accurate.\n",
    "\n",
    "You may be wondering: What is LSTM? Here's a brief explanation:\n",
    "\n",
    "## What is LSTM?\n",
    "\n",
    "LSTM stands for Long Short-Term Memory, which is based off a RNN (Recurrent Neural Network). A recurring neural network works very similarly to how our brains work, looking back on previous data from a short time period and predicting what will happen in the future based on that data.\n",
    "\n",
    "## The Problem: RNN\n",
    "\n",
    "An RNN is a Recurrent Neural Network, which is able to look through a short amount of information and make inferences based on this information. Keywords: short-term memory. An RNN is simply unable to handle the amount of information required for stock prediction because it only works on a short term basis. Because of this, very important information from the beginning is left out, which is essential to successfully predicting stock data.\n",
    "\n",
    "This is largely due to back propagation, which in short is the gradient value (the values used to update the neural networks weights) disappearing as time goes on. When the gradient value becomes smaller and smaller, this results in not much learning happening based on the data to which the weight is assigned to. This leads to that data being ignored and therefore not being used later on to accurately predict the future data. \n",
    "\n",
    "## The Solution: LSTMs and GRUs\n",
    "![](https://miro.medium.com/v2/resize:fit:720/format:webp/1*yBXV9o5q7L_CvY7quJt3WQ.png)\n",
    "\n",
    "In the image above, you can see how an LSTM and GRU functions. Ignore the GRU portion, as it is not used in our model.\n",
    "\n",
    "Going back to how an RNN functions, it takes in previous data, in the form of vectors, which are passed through the RNN processes.\n",
    "![](https://miro.medium.com/v2/resize:fit:720/format:webp/1*AQ52bwW55GsJt6HTxPDuMA.gif)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%maven org.nd4j:nd4j-native-platform:1.0.0-beta2\n",
    "%maven org.deeplearning4j:deeplearning4j-core:1.0.0-beta2\n",
    "%maven org.deeplearning4j:deeplearning4j-ui_2.11:1.0.0-beta2\n",
    "%maven ch.qos.logback:logback-classic:1.2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import java.io.BufferedReader;\n",
    "import java.io.File;\n",
    "import java.io.FileReader;\n",
    "import java.io.IOException;\n",
    "import java.util.ArrayList;\n",
    "\n",
    "import org.deeplearning4j.eval.ROC;\n",
    "import org.deeplearning4j.nn.multilayer.MultiLayerNetwork;\n",
    "import org.deeplearning4j.util.ModelSerializer;\n",
    "import org.nd4j.linalg.api.ndarray.INDArray;\n",
    "import org.nd4j.linalg.dataset.DataSet;\n",
    "import org.nd4j.linalg.dataset.api.iterator.DataSetIterator;\n",
    "import org.nd4j.linalg.dataset.api.preprocessor.NormalizerMinMaxScaler;\n",
    "import org.nd4j.linalg.factory.Nd4j;\n",
    "\n",
    "\n",
    "import org.jfree.data.category.CategoryDataset;\n",
    "import org.jfree.data.category.DefaultCategoryDataset;\n",
    "\n",
    "import com.nighthawk.spring_portfolio.mvc.lstm.LSTMGraph;\n",
    "\n",
    "import lombok.AllArgsConstructor;\n",
    "import lombok.Data;\n",
    "import lombok.NoArgsConstructor;\n",
    "import lombok.ToString;\n",
    "\n",
    "@NoArgsConstructor\n",
    "@AllArgsConstructor\n",
    "@ToString\n",
    "@Data\n",
    "public class LSTMTrainerTester {\n",
    "\n",
    "    private String file;\n",
    "    private double splitRatio;\n",
    "    private int labelIndex;\n",
    "    private int features; //5\n",
    "    private int labels; // 1\n",
    "    private int batchSize; //32\n",
    "    private int stepCount; //1\n",
    "    private ROC roc;\n",
    "    private DataSetIterator iterator;\n",
    "\n",
    "    public LSTMTrainerTester(String directory, String ticker, int features, int labels, int batchSize, int stepCount) {\n",
    "        this.file = directory + \"/\" + ticker + \"_test.csv\";\n",
    "        this.features = features;\n",
    "        this.labels = labels;\n",
    "        this.batchSize = batchSize;\n",
    "        this.stepCount = stepCount;\n",
    "        this.roc = new ROC(100);\n",
    "    }\n",
    "\n",
    "    public void TrainAndTestModel(MultiLayerNetwork net) {\n",
    "        \n",
    "        ArrayList<Double> actual = new ArrayList<Double>();\n",
    "        ArrayList<Double> predicted = new ArrayList<Double>();\n",
    "        NormalizerMinMaxScaler minMaxScaler = new NormalizerMinMaxScaler(0,1);\n",
    "        try (BufferedReader br = new BufferedReader(new FileReader(file))) {\n",
    "            String line = br.readLine();\n",
    "            System.out.println(line);\n",
    "            for (int i = 0; i < 32*100; i++) {\n",
    "                line = br.readLine();\n",
    "            }\n",
    "            for (int i = 0; i < 20; i ++) {\n",
    "                double[][][] featureMatrix = new double[batchSize][this.features][this.stepCount];\n",
    "                double[][][] labelsMatrix = new double[batchSize][this.labels][this.stepCount];\n",
    "                for (int batch = 0; batch < this.batchSize; batch++) {\n",
    "                    line = br.readLine();\n",
    "                    String[] values = line.split(\",\");\n",
    "                    featureMatrix[batch][0][0] = Double.parseDouble(values[1]); // OPEN\n",
    "                    featureMatrix[batch][1][0] = Double.parseDouble(values[2]); // HIGH\n",
    "                    featureMatrix[batch][2][0] = Double.parseDouble(values[3]); // LOW\n",
    "                    labelsMatrix[batch][0][0] = Double.parseDouble(values[4]); // CLOSE\n",
    "                }\n",
    "                INDArray featuresArray = Nd4j.create(featureMatrix);\n",
    "                INDArray labelsArray = Nd4j.create(labelsMatrix);\n",
    "                DataSet train = new DataSet(featuresArray, labelsArray);\n",
    "                minMaxScaler.fit(train);\n",
    "                // System.out.println(train);\n",
    "                net.fit(train);\n",
    "                net.rnnClearPreviousState();\n",
    "            }\n",
    "            for (int i = 0; i<1; i++) {\n",
    "                double[][][] featureMatrix = new double[batchSize][this.features][this.stepCount];\n",
    "                double[][][] labelsMatrix = new double[batchSize][this.labels][this.stepCount];\n",
    "                for (int batch = 0; batch < this.batchSize; batch++) {\n",
    "                    line = br.readLine();\n",
    "                    String[] values = line.split(\",\");\n",
    "                    featureMatrix[batch][0][0] = Double.parseDouble(values[1]); // OPEN\n",
    "                    featureMatrix[batch][1][0] = Double.parseDouble(values[2]); // HIGH\n",
    "                    featureMatrix[batch][2][0] = Double.parseDouble(values[3]); // LOW\n",
    "                    labelsMatrix[batch][0][0] = Double.parseDouble(values[4]); // CLOSE\n",
    "                }\n",
    "                INDArray featuresArray = Nd4j.create(featureMatrix);\n",
    "                INDArray labelsArray = Nd4j.create(labelsMatrix);\n",
    "                DataSet test = new DataSet(featuresArray, labelsArray);\n",
    "                minMaxScaler.fit(test);\n",
    "                INDArray output = net.output(test.getFeatures());\n",
    "                for (int j = 0; j < this.batchSize; j++) {\n",
    "                    actual.add(test.getLabels().getDouble(j,0,0));\n",
    "                    predicted.add(output.getDouble(j,0,0));\n",
    "                }\n",
    "                roc.evalTimeSeries(test.getLabels(), output);\n",
    "            }\n",
    "            System.out.println(\"Output: \");\n",
    "            System.out.println(predicted);\n",
    "            System.out.println(\"Actual: \");\n",
    "            System.out.println(actual);\n",
    "            \n",
    "            System.out.println(\"FINAL TEST AUC: \" + roc.calculateAUC());\n",
    "            File locationToSave = new File(\"src/main/java/com/nighthawk/spring_portfolio/mvc/lstm/resources/StockPriceLSTM_\".concat(\"CLOSE\").concat(\".zip\"));\n",
    "            ModelSerializer.writeModel(net, locationToSave, true);\n",
    "            LSTMGraph plotter = new LSTMGraph(actual, predicted);\n",
    "            System.out.println(\"Image created\");\n",
    "            net = ModelSerializer.restoreMultiLayerNetwork(locationToSave);\n",
    "        } catch (IOException e) {\n",
    "            e.printStackTrace();\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing code for LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.deeplearning4j.datasets.iterator.impl.*;\n",
    "import org.deeplearning4j.nn.api.OptimizationAlgorithm;\n",
    "import org.deeplearning4j.nn.conf.GradientNormalization;\n",
    "import org.deeplearning4j.nn.conf.NeuralNetConfiguration;\n",
    "import org.deeplearning4j.nn.conf.layers.LSTM;\n",
    "import org.deeplearning4j.nn.conf.layers.RnnOutputLayer;\n",
    "import org.deeplearning4j.nn.multilayer.MultiLayerNetwork;\n",
    "import org.deeplearning4j.nn.weights.WeightInit;\n",
    "import org.deeplearning4j.optimize.listeners.ScoreIterationListener;\n",
    "import org.nd4j.linalg.activations.Activation;\n",
    "import org.nd4j.linalg.dataset.api.iterator.DataSetIterator;\n",
    "import org.nd4j.linalg.learning.config.Adam;\n",
    "import org.nd4j.linalg.lossfunctions.LossFunctions;\n",
    "import java.util.ArrayList;\n",
    "import java.util.List;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "CompilationException",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1m\u001b[30m|   \u001b[1m\u001b[30mpublic class LstmExample {\u001b[0m",
      "\u001b[1m\u001b[30m|   \u001b[1m\u001b[30m\u001b[0m",
      "\u001b[1m\u001b[30m|   \u001b[1m\u001b[30m    public static void main(String[] args) {\u001b[0m",
      "\u001b[1m\u001b[30m|   \u001b[1m\u001b[30m        // Define your dataset and load it\u001b[0m",
      "\u001b[1m\u001b[30m|   \u001b[1m\u001b[30m        // Example: List<double[]> features = loadFeatures();\u001b[0m",
      "\u001b[1m\u001b[30m|   \u001b[1m\u001b[30m        // List<double[]> labels = loadLabels();\u001b[0m",
      "\u001b[1m\u001b[30m|   \u001b[1m\u001b[30m\u001b[0m",
      "\u001b[1m\u001b[30m|   \u001b[1m\u001b[30m        // Create DataSetIterator\u001b[0m",
      "\u001b[1m\u001b[30m|   \u001b[1m\u001b[30m        DataSetIterator iterator = new ListDataSetIterator<>(createData(features, labels));\u001b[0m",
      "\u001b[1m\u001b[30m|   \u001b[1m\u001b[30m\u001b[0m",
      "\u001b[1m\u001b[30m|   \u001b[1m\u001b[30m        // Neural network configuration\u001b[0m",
      "\u001b[1m\u001b[30m|   \u001b[1m\u001b[30m        MultiLayerNetwork model = new MultiLayerNetwork(new NeuralNetConfiguration.Builder()\u001b[0m",
      "\u001b[1m\u001b[30m|   \u001b[1m\u001b[30m                .seed(123)\u001b[0m",
      "\u001b[1m\u001b[30m|   \u001b[1m\u001b[30m                .optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT)\u001b[0m",
      "\u001b[1m\u001b[30m|   \u001b[1m\u001b[30m                .updater(new Adam(0.01))\u001b[0m",
      "\u001b[1m\u001b[30m|   \u001b[1m\u001b[30m                .l2(1e-4)\u001b[0m",
      "\u001b[1m\u001b[30m|   \u001b[1m\u001b[30m                .weightInit(WeightInit.XAVIER)\u001b[0m",
      "\u001b[1m\u001b[30m|   \u001b[1m\u001b[30m                .gradientNormalization(GradientNormalization.ClipElementWiseAbsoluteValue)\u001b[0m",
      "\u001b[1m\u001b[30m|   \u001b[1m\u001b[30m                .gradientNormalizationThreshold(0.5)\u001b[0m",
      "\u001b[1m\u001b[30m|   \u001b[1m\u001b[30m                .list()\u001b[0m",
      "\u001b[1m\u001b[30m|   \u001b[1m\u001b[30m                .layer(new LSTM.Builder()\u001b[0m",
      "\u001b[1m\u001b[30m|   \u001b[1m\u001b[30m                        .nIn(numInputs)\u001b[0m",
      "\u001b[1m\u001b[30m|   \u001b[1m\u001b[30m                        .nOut(numHiddenUnits)\u001b[0m",
      "\u001b[1m\u001b[30m|   \u001b[1m\u001b[30m                        .activation(Activation.TANH)\u001b[0m",
      "\u001b[1m\u001b[30m|   \u001b[1m\u001b[30m                        .build())\u001b[0m",
      "\u001b[1m\u001b[30m|   \u001b[1m\u001b[30m                .layer(new RnnOutputLayer.Builder(LossFunctions.LossFunction.MSE)\u001b[0m",
      "\u001b[1m\u001b[30m|   \u001b[1m\u001b[30m                        .activation(Activation.IDENTITY)\u001b[0m",
      "\u001b[1m\u001b[30m|   \u001b[1m\u001b[30m                        .nIn(numHiddenUnits)\u001b[0m",
      "\u001b[1m\u001b[30m|   \u001b[1m\u001b[30m                        .nOut(numOutputs)\u001b[0m",
      "\u001b[1m\u001b[30m|   \u001b[1m\u001b[30m                        .build())\u001b[0m",
      "\u001b[1m\u001b[30m|   \u001b[1m\u001b[30m                .pretrain(false)\u001b[0m",
      "\u001b[1m\u001b[30m|   \u001b[1m\u001b[30m                .backprop(true)\u001b[0m",
      "\u001b[1m\u001b[30m|   \u001b[1m\u001b[30m                .build()\u001b[0m",
      "\u001b[1m\u001b[30m|   \u001b[1m\u001b[30m        );\u001b[0m",
      "\u001b[1m\u001b[30m|   \u001b[1m\u001b[30m\u001b[0m",
      "\u001b[1m\u001b[30m|   \u001b[1m\u001b[30m        model.init();\u001b[0m",
      "\u001b[1m\u001b[30m|   \u001b[1m\u001b[30m        model.setListeners(new ScoreIterationListener(20));  // Print the score with every 20 iterations\u001b[0m",
      "\u001b[1m\u001b[30m|   \u001b[1m\u001b[30m\u001b[0m",
      "\u001b[1m\u001b[30m|   \u001b[1m\u001b[30m        // Train the model\u001b[0m",
      "\u001b[1m\u001b[30m|   \u001b[1m\u001b[30m        int numEpochs = 50;\u001b[0m",
      "\u001b[1m\u001b[30m|   \u001b[1m\u001b[30m        for (int i = 0; i < numEpochs; i++) {\u001b[0m",
      "\u001b[1m\u001b[30m|   \u001b[1m\u001b[30m            model.fit(iterator);\u001b[0m",
      "\u001b[1m\u001b[30m|   \u001b[1m\u001b[30m        }\u001b[0m",
      "\u001b[1m\u001b[30m|   \u001b[1m\u001b[30m\u001b[0m",
      "\u001b[1m\u001b[30m|   \u001b[1m\u001b[30m        // Your model is now trained and ready for prediction\u001b[0m",
      "\u001b[1m\u001b[30m|   \u001b[1m\u001b[30m    }\u001b[0m",
      "\u001b[1m\u001b[30m|   \u001b[1m\u001b[30m\u001b[0m",
      "\u001b[1m\u001b[30m|   \u001b[1m\u001b[30m    private static List<DataSet> createData(List<double[]> features, List<double[]> labels) {\u001b[0m",
      "\u001b[1m\u001b[30m|   \u001b[1m\u001b[30m        List<DataSet> dataSets = new ArrayList<>();\u001b[0m",
      "\u001b[1m\u001b[30m|   \u001b[1m\u001b[30m        for (int i = 0; i < features.size(); i++) {\u001b[0m",
      "\u001b[1m\u001b[30m|   \u001b[1m\u001b[30m            dataSets.add(new DataSet(Nd4j.create(features.get(i)), Nd4j.create(labels.get(i))));\u001b[0m",
      "\u001b[1m\u001b[30m|   \u001b[1m\u001b[30m        }\u001b[0m",
      "\u001b[1m\u001b[30m|   \u001b[1m\u001b[30m        return dataSets;\u001b[0m",
      "\u001b[1m\u001b[30m|   \u001b[1m\u001b[30m    }\u001b[0m",
      "\u001b[1m\u001b[30m|   \u001b[1m\u001b[30m\u001b[0m",
      "\u001b[1m\u001b[30m|   \u001b[1m\u001b[30m    // Implement methods to load your features and labels from your dataset\u001b[0m",
      "\u001b[1m\u001b[30m|   \u001b[1m\u001b[30m    // private static List<double[]> loadFeatures() { ... }\u001b[0m",
      "\u001b[1m\u001b[30m|   \u001b[1m\u001b[30m    // private static List<double[]> loadLabels() { ... }\u001b[0m",
      "\u001b[1m\u001b[30m|   \u001b[1m\u001b[30m}\u001b[0m",
      "\u001b[1m\u001b[31mUnresolved dependencies:\u001b[0m",
      "\u001b[1m\u001b[31m   - class DataSet\u001b[0m",
      "\u001b[1m\u001b[31m   - class ListDataSetIterator\u001b[0m",
      "\u001b[1m\u001b[31m   - variable features\u001b[0m",
      "\u001b[1m\u001b[31m   - variable labels\u001b[0m",
      "\u001b[1m\u001b[31m   - variable numHiddenUnits\u001b[0m",
      "\u001b[1m\u001b[31m   - variable numInputs\u001b[0m",
      "\u001b[1m\u001b[31m   - variable numOutputs\u001b[0m",
      "\u001b[1m\u001b[31m   - variable Nd4j\u001b[0m"
     ]
    }
   ],
   "source": [
    "public class LstmExample {\n",
    "\n",
    "    public static void main(String[] args) {\n",
    "        // Define your dataset and load it\n",
    "        // Example: List<double[]> features = loadFeatures();\n",
    "        // List<double[]> labels = loadLabels();\n",
    "\n",
    "        // Create DataSetIterator\n",
    "        DataSetIterator iterator = new ListDataSetIterator<>(createData(features, labels));\n",
    "\n",
    "        // Neural network configuration\n",
    "        MultiLayerNetwork model = new MultiLayerNetwork(new NeuralNetConfiguration.Builder()\n",
    "                .seed(123)\n",
    "                .optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT)\n",
    "                .updater(new Adam(0.01))\n",
    "                .l2(1e-4)\n",
    "                .weightInit(WeightInit.XAVIER)\n",
    "                .gradientNormalization(GradientNormalization.ClipElementWiseAbsoluteValue)\n",
    "                .gradientNormalizationThreshold(0.5)\n",
    "                .list()\n",
    "                .layer(new LSTM.Builder()\n",
    "                        .nIn(numInputs)\n",
    "                        .nOut(numHiddenUnits)\n",
    "                        .activation(Activation.TANH)\n",
    "                        .build())\n",
    "                .layer(new RnnOutputLayer.Builder(LossFunctions.LossFunction.MSE)\n",
    "                        .activation(Activation.IDENTITY)\n",
    "                        .nIn(numHiddenUnits)\n",
    "                        .nOut(numOutputs)\n",
    "                        .build())\n",
    "                .pretrain(false)\n",
    "                .backprop(true)\n",
    "                .build()\n",
    "        );\n",
    "\n",
    "        model.init();\n",
    "        model.setListeners(new ScoreIterationListener(20));  // Print the score with every 20 iterations\n",
    "\n",
    "        // Train the model\n",
    "        int numEpochs = 50;\n",
    "        for (int i = 0; i < numEpochs; i++) {\n",
    "            model.fit(iterator);\n",
    "        }\n",
    "\n",
    "        // Your model is now trained and ready for prediction\n",
    "    }\n",
    "\n",
    "    private static List<DataSet> createData(List<double[]> features, List<double[]> labels) {\n",
    "        List<DataSet> dataSets = new ArrayList<>();\n",
    "        for (int i = 0; i < features.size(); i++) {\n",
    "            dataSets.add(new DataSet(Nd4j.create(features.get(i)), Nd4j.create(labels.get(i))));\n",
    "        }\n",
    "        return dataSets;\n",
    "    }\n",
    "\n",
    "    // Implement methods to load your features and labels from your dataset\n",
    "    // private static List<double[]> loadFeatures() { ... }\n",
    "    // private static List<double[]> loadLabels() { ... }\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "java",
   "language": "java",
   "name": "java"
  },
  "language_info": {
   "codemirror_mode": "java",
   "file_extension": ".jshell",
   "mimetype": "text/x-java-source",
   "name": "java",
   "pygments_lexer": "java",
   "version": "11.0.21+9-post-Ubuntu-0ubuntu122.04"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
